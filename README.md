# arafix_ocr

## Introduction
This tool improves the output of generic OCR systems by utilizing an n-gram based post-correction approach. While most techniques that seek to improve Arabic OCR output focus on the Computer Vision aspect of converting image to text, the post correction module in our tool focuses on taking in the flawed output of one such image to text system and then improving on it without any knowledge of the image.

In addition to the post-correction system, this repo contains modules that 
1) Utilize an external OCR Api to convert image to text
2) Evaluate the quality of results of the system on the word level (when the ground truth is known)

## How it works
We first "encode" a given line of text using the following rule: every token is composed of one letter + a hash each in the direction in which it is connected to another character. So for example:
I am an apple -> I a# #m a# #n a# #p# #p# #l# #e

This encoding has been chosen to address the segmentation issues in Arabic OCR. 

After the line has been encoded, it is run through the main "prediction" tool, which determines if any token needs to be changed based on the tokens that preceed it (n-gram). Finally, the output from the prediction step is decoded by removing the #'s and joining the characters based on space specification. The decoded output is then compared with the ground truth to see if improvements have been made. 

Note: occasionally, the predicted output for a line will contain an impossible scenario such as: A #l# #a. Here, the 'A' token says that it is completely independent (A la), but the '#l#' that follows it says that it should be connected to the 'A' (Ala). In these cases, the default decoding decision is to split the word.

## Installation Guide
<!-- 
- Download srilm: Navigate to this [link](http://www.speech.sri.com/projects/srilm/download.html) and download version of 1.7.3 of srilm into the main directory of this repo -->
- Run the following commands:

  ```cd code```
  
  ```sh install.sh```
  
  The previous commands will install all the required dependencies for arafix. The tool should be ready to use!
  
## Usage

Arafix has 3 main modules:
1) image_to_text: converts image to text and searchable pdf
2) predict: improves the generated text from previous module
3) evaluate: evaluates the word error rate of outputs from the first and second modules

IMPORTANT NOTE: As of now, the predict module is not ready for use. Thus, all related commands in arafix have been commented out. Given the current functionality, a user can scan an image and evaluate its accuracy but the output cannot be improved using the predict module.

To run arafix, do the following:
1) Open the data folder and create a subfolder with the name of the book you intend to run arafix on
2) Within the book's subfolder, create a subfolder named <book_name>_raw_ocr
3) Within the <book_name>_raw_ocr subfolder, add all the pages of the respective book as .tif files
4) Optionally, if you intend to perform evaluation of your result (only if you have ground truth), create another subfolder within the <book_name> folder called <book_name>_ground_truth. This folder should contain the ground truth text files for your book (one file for every page). 
5) Open code/arafix.sh in a text editor
6) Modify variables* as needed
7) Open terminal and navigate to arafix_ocr
8) ```cd code```
9) ```sh arafix.sh 'book_name'```

*arafix.sh variables:
- config_name: which config file should arafix read the settings from
- start_page: which page should arafix start running from. Set it to "None" to run it from the lowest possible page.
- end_page: which page should arafix run till (inclusive). Set it to "None" to run till the highest possible page.


### [Optional] Configuration

Arafix runs with the default settings as described in the configs/default.txt file. If you wish to modify these settings, do the following:
1) Create a copy of default.txt and modify the parameters within this file
2) Update arafix.sh with the name of the new config file in config_name variable.

Configuration Parameters:
- image_to_text
  - api_key: Obtained from ocr.space, a commercial scanning software
  - skip_converted (True/False): If true, it skips files which have already been converted to save API calls
  - create_pdf (True/False): If true, it creates searchable pdfs as well

- predict
  - map_name (check mappings directory for possible options): Different mapping files are used to fix different kinds of errors
  - model_name (m1.lm/m2.lm): m1 is more accuracte but slower 
  - order (1-8): the tool performs best at order 8. if it's lower, it will take lesser time to execute at the cost of reduced accuracy
  - keep_scratch (True/False): if set to False, it deletes all the scratch files generated during prediction
  - create_pdf (True/False): if set to true, it will use fix the errors in the searchable pdf created in the previous module.

- evaluate

  select the next 3 parameters depending on the results you would like to evaluate (e.g. if you fixed errors using order 8 and segmenter mapping then select the same in this step to carry out its evaluation)
  
  - map_name 
  - model_name
  - order
  - keep_scratch (True/False): if set to False, it deletes all the scratch files generated during evaluation


## File Organization
```
ðŸ“¦code 
 â”£ ðŸ“‚ced_word_alignment /
 â”£ ðŸ“‚srilm-1.7.3 
 â”£ ðŸ“‚utils 
 â”ƒ â”£ ðŸ“œMakefile.machine.macosx 
 â”ƒ â”— ðŸ“œdependencies.txt
 â”£ ðŸ“œalign_text.py
 â”£ ðŸ“œalignment.py
 â”£ ðŸ“œarafix.sh 
 â”£ ðŸ“œevaluate.py 
 â”£ ðŸ“œimage_to_text.py 
 â”£ ðŸ“œinstall.sh 
 â”— ðŸ“œpredict.py 

ðŸ“¦configs 
 â”— ðŸ“œdefault.txt
 
ðŸ“¦mappings 
 â”£ ðŸ“‚empirical
 â”ƒ â”£ ðŸ“œemp_space_perc_f5_pg_1_top_10.map
 â”ƒ â”— ðŸ“œemp_space_perc_f5_pg_1_top_5.map
 â”£ ðŸ“‚visual
 â”ƒ â”£ ðŸ“œvisual_type_1_prob_40top_6.map
 â”ƒ â”— ðŸ“œvisual_type_1_prob_40top_7.map
 â”— ðŸ“œsegmenter.map
 
ðŸ“¦models 
 â”— ðŸ“œm2.lm
 
ðŸ“¦data 
â”— ðŸ“‚princeton_aco001005_hi
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_alignment 
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_ground_truth 
â”ƒ â”ƒ â”£ ðŸ“œground_truth_1.txt
â”ƒ â”ƒ â”£ ðŸ“œground_truth_2.txt
â”ƒ â”ƒ â”— ðŸ“œground_truth_3.txt
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_post_edited
â”ƒ â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_model_m2_map_segmenter
â”ƒ â”ƒ â”ƒ â”£ ðŸ“œpredicted_1.txt
â”ƒ â”ƒ â”ƒ â”£ ðŸ“œpredicted_2.txt
â”ƒ â”ƒ â”ƒ â”— ðŸ“œpredicted_3.txt
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_raw_embed_pdf
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_raw_images
â”ƒ â”ƒ â”£ ðŸ“œprinceton_aco001005_n000001_d.tif
â”ƒ â”ƒ â”£ ðŸ“œprinceton_aco001005_n000002_d.tif
â”ƒ â”ƒ â”— ðŸ“œprinceton_aco001005_n000003_d.tif
â”ƒ â”£ ðŸ“‚princeton_aco001005_hi_raw_ocr
â”ƒ â”ƒ â”£ ðŸ“œocr_space_output_1.txt
â”ƒ â”ƒ â”£ ðŸ“œocr_space_output_2.txt
â”ƒ â”ƒ â”— ðŸ“œocr_space_output_3.txt
```

## Technical Documentation
  - Versioning: As for python, version 3.8.3 was used. utils/dependencies.txt and install.sh contain the required packages with their respective versions.
  - Models: The models were built using the ```ngram-count``` function in the SRILM toolkit. The following specfications were used:
    - Order: 8
    - Smoothing: Kneser-Ney
    - keep-unk: True

  - arafix.sh: This bash script is the main function to be executed. It calls the 3 main modules of arafix tool. arafix_dalma.sh provides the same code but with dalma compatibility
  - image_to_text.py: This module uses the OCR Space API to convert images into text. It also stores relevant JSON info of the OCR'ed files. The settings for the API calls can be modified within ocr_space_func() 
  - predict.py: This module does the following:
    - encode the input text
    - pass the encoded text to ```disambig``` function of SRILM toolkit
    - decode the text outputted by disambig
  - evaluate.py: This module uses ced_word_alignment tool to align the ground truth against ocr and predicted. Then it calculates word error rate using the following formula: (subs + deletions + insertions) / (subs + deletions + correct words) 
  
